# https://taskfile.dev

version: '3'

vars:
  GENERATED_BENCHMARK_PATH: ./tasks_output/generated_benchmarks

  STRIX_RESULTS_PATH: ./tasks_output/strix
  SPOT_MODULAR_RESULTS_PATH: ./tasks_output/spot_modular
  DEPSYNT_RESULTS_PATH: ./tasks_output/depsynt
  DEPSYNT_MEASURES_RESULTS_PATH: ./tasks_output/depsynt-measures

  STRIX_SUMMARY: ./tasks_output/strix.csv
  DEPSYNT_SUMMARY: ./tasks_output/depsynt.csv
  DEPSYNT_MEASURE_SUMMARY: ./tasks_output/depsynt_measure.csv
  SPOT_MODULAR_SUMMARY: ./tasks_output/spotmodular.csv

  SYNTHESIS_TIMEOUT: 180m
  BENCHMARKS_FAMALIES: "tsl_smart_home_jarvis/extracted-benchmarks,tsl_paper,mux,ltl2dpa,shift"

  FIND_DEPENDENCIES_PATH: ./tasks_output/find_deps
  FIND_DEPENDENCIES_SUMMARY: ./tasks_output/find_deps.csv
  FIND_DEPENDENCIES_TIMEOUT: 60m

tasks:
  # Synthesis
  depsynt:
    desc: Run DepSynt tool on the generated benchmarks
    summary: |
      This task removes the previous results and submits a new job to the cluster.
      The job is generated using the `slurm_task_gen.py` script with the args of:
      synthesis timeout, benchmarks text files path, the output directory to put the results on, what families of benchmark to run the results on.
    cmds:
      - mkdir -p {{.DEPSYNT_RESULTS_PATH}}
      - rm -f {{.DEPSYNT_RESULTS_PATH}}/*
      - rm -f ./submit_depsynt
      - chmod +x ./scripts/slurm_task_gen.py
      - ./scripts/slurm_task_gen.py --task=depsynt --timeout={{.SYNTHESIS_TIMEOUT}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --output-path={{.DEPSYNT_RESULTS_PATH}} --families="{{.BENCHMARKS_FAMALIES}}" > submit_depsynt
      - sbatch submit_depsynt

  # Synthesis
  depsynt:measure:
    desc: Run DepSynt tool with extra measures
    cmds:
      - mkdir -p {{.DEPSYNT_MEASURES_RESULTS_PATH}}
      - rm -f {{.DEPSYNT_MEASURES_RESULTS_PATH}}/*
      - rm -f ./submit_depsynt_measures
      - chmod +x ./scripts/slurm_task_gen.py
      - ./scripts/slurm_task_gen.py --task=depsynt_measured --timeout={{.SYNTHESIS_TIMEOUT}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --output-path={{.DEPSYNT_MEASURES_RESULTS_PATH}} --families="{{.BENCHMARKS_FAMALIES}}" > submit_depsynt_measures
      - sbatch submit_depsynt_measures

  depsynt:summary:
    cmds:
      - rm -f {{.DEPSYNT_SUMMARY}}
      - chmod +x ./scripts/results_summarizer.py
      - ./scripts/results_summarizer.py --result-path={{.DEPSYNT_RESULTS_PATH}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --summary-output={{.DEPSYNT_SUMMARY}} --tool=depsynt

  depsynt:measure:summary:
    cmds:
      - rm -f {{.DEPSYNT_MEASURE_SUMMARY}}
      - chmod +x ./scripts/results_summarizer.py
      - ./scripts/results_summarizer.py --result-path={{.DEPSYNT_MEASURES_RESULTS_PATH}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --summary-output={{.DEPSYNT_MEASURE_SUMMARY}} --tool=depsynt

  spot_modular:
    - mkdir -p {{.SPOT_MODULAR_RESULTS_PATH}}
    - rm -f {{.SPOT_MODULAR_RESULTS_PATH}}/*
    - rm -f ./submit_spot_modular
    - chmod +x ./scripts/slurm_task_gen.py
    - ./scripts/slurm_task_gen.py --task=spotmodular --timeout={{.SYNTHESIS_TIMEOUT}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --output-path={{.SPOT_MODULAR_RESULTS_PATH}} --families="{{.BENCHMARKS_FAMALIES}}" > submit_spot_modular
    - sbatch submit_spot_modular

  spot_modular:summary:
    cmds:
      - rm -f {{.SPOT_MODULAR_SUMMARY}}
      - chmod +x ./scripts/results_summarizer.py
      - ./scripts/results_summarizer.py --result-path={{.SPOT_MODULAR_RESULTS_PATH}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --summary-output={{.SPOT_MODULAR_SUMMARY}} --tool=depsynt

  cactus:
    desc: Generate Cactus Plot from the results of the synthesis
    cmds:
      - python ./scripts/generate_cactus.py

  strix:
    cmds:
      - mkdir -p {{.STRIX_RESULTS_PATH}}
      - rm -f {{.STRIX_RESULTS_PATH}}/*
      - rm -f ./submit_strix
      - chmod +x ./scripts/slurm_task_gen.py
      - ./scripts/slurm_task_gen.py --task=strix --timeout={{.SYNTHESIS_TIMEOUT}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --output-path={{.STRIX_RESULTS_PATH}} --families="{{.BENCHMARKS_FAMALIES}}" > submit_strix
      - sbatch submit_strix

  strix:summary:
    cmds:
      - rm -f {{.STRIX_SUMMARY}}
      - chmod +x ./scripts/results_summarizer.py
      - ./scripts/results_summarizer.py --result-path={{.STRIX_RESULTS_PATH}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --summary-output={{.STRIX_SUMMARY}} --tool=strix




  # Find Dependencies
  find_dependencies:
    cmds:
      - mkdir -p {{.FIND_DEPENDENCIES_PATH}}
      - rm -f {{.FIND_DEPENDENCIES_PATH}}/*
      - rm -f ./submit_find_deps
      - chmod +x ./scripts/slurm_task_gen.py
      - ./scripts/slurm_task_gen.py --task=find_deps --timeout={{.FIND_DEPENDENCIES_TIMEOUT}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --output-path={{.FIND_DEPENDENCIES_PATH}} > submit_find_deps
      - sbatch submit_find_deps

  find_dependencies:summary:
    - rm -f {{.FIND_DEPENDENCIES_SUMMARY}}
    - chmod +x ./scripts/results_summarizer.py
    - ./scripts/results_summarizer.py --result-path={{.FIND_DEPENDENCIES_PATH}} --benchmarks-path={{.GENERATED_BENCHMARK_PATH}}/text --summary-output={{.FIND_DEPENDENCIES_SUMMARY}} --tool=find_deps

  # Generate benchmarks
  generate_benchmarks:
    cmds:
      - task: generate_benchmarks:clean
      - task: generate_benchmarks:tlsf
      - task: generate_benchmarks:text

  generate_benchmarks:tlsf:
    internal: true
    cmds:
      - mkdir -p {{.GENERATED_BENCHMARK_PATH}}/tlsf
      - chmod +x ./scripts/benchmarks/tlsf/selectBenchmarks.py
      - ./scripts/benchmarks/tlsf/selectBenchmarks.py ./scripts/benchmarks/tlsf {{.GENERATED_BENCHMARK_PATH}}/tlsf --savestruct
  generate_benchmarks:clean:
    internal: true
    cmds:
      - rm -rf {{.GENERATED_BENCHMARK_PATH}}
      - mkdir -p {{.GENERATED_BENCHMARK_PATH}}
  generate_benchmarks:text:
    internal: true
    deps: [generate_benchmarks:tlsf]
    cmds:
      - rm -rf {{.GENERATED_BENCHMARK_PATH}}/text
      - mkdir -p {{.GENERATED_BENCHMARK_PATH}}/text
      - chmod +x ./scripts/tlsf_to_text.py
      - ./scripts/tlsf_to_text.py {{.GENERATED_BENCHMARK_PATH}}/tlsf {{.GENERATED_BENCHMARK_PATH}}/text

  clean_skipped_synthesis:  # Example: task clean_skipped_synthesis CLEAN_PATH=./tasks_output/depsynt
    desc: Remove the skipped synthesis results
    summary: |
      Benchmarks may be skipped since it wasn't defined in the families of benchmarks to run on.
      The script `remove_skipped_synt.sh` removes the results of the synthesis that were skipped.
      The script takes as an argument the path to the results directory.
    cmds:
      - chmod +x ./scripts/remove_skipped_synt.sh
      - ./scripts/remove_skipped_synt.sh {{.CLEAN_PATH}}

